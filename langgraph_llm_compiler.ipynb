{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/langchain-ai/langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20627A1i-Nlz",
        "outputId": "0b71d78c-5e70-48f8-9477-13a428b1e935"
      },
      "id": "20627A1i-Nlz",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'langgraph'...\n",
            "remote: Enumerating objects: 37129, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 37129 (delta 53), reused 34 (delta 31), pack-reused 37031 (from 3)\u001b[K\n",
            "Receiving objects: 100% (37129/37129), 469.51 MiB | 30.15 MiB/s, done.\n",
            "Resolving deltas: 100% (26244/26244), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/langgraph/docs/docs/tutorials/llm-compiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOcqx3548E8K",
        "outputId": "ff4d8345-a06d-4078-bf8e-9f286cc41206"
      },
      "id": "dOcqx3548E8K",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/langgraph/docs/docs/tutorials/llm-compiler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8b472b-f3fb-46c2-841f-930a4692697b",
      "metadata": {
        "id": "0c8b472b-f3fb-46c2-841f-930a4692697b"
      },
      "source": [
        "# LLMCompiler\n",
        "\n",
        "This notebook shows how to implement [LLMCompiler, by Kim, et. al](https://arxiv.org/abs/2312.04511) in LangGraph.\n",
        "\n",
        "LLMCompiler is an agent architecture designed to **speed up** the execution of agentic tasks by eagerly-executed tasks within a DAG. It also saves costs on redundant token usage by reducing the number of calls to the LLM. Below is an overview of its computational graph:\n",
        "\n",
        "![LLMCompiler Graph](attachment:52710d04-a318-4e3c-8457-eceb4b422d5d.png)\n",
        "\n",
        "It has 3 main components:\n",
        "\n",
        "1. Planner: stream a DAG of tasks.\n",
        "2. Task Fetching Unit: schedules and executes the tasks as soon as they are executable\n",
        "3. Joiner: Responds to the user or triggers a second plan\n",
        "\n",
        "\n",
        "This notebook walks through each component and shows how to wire them together using LangGraph. The end result will leave a trace [like the following](https://smith.langchain.com/public/218c2677-c719-4147-b0e9-7bc3b5bb2623/r).\n",
        "\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required packages and set our API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "16bd5497-35ad-44f2-94d9-19ff39a5ffed",
      "metadata": {
        "id": "16bd5497-35ad-44f2-94d9-19ff39a5ffed"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U --quiet langchain_openai langsmith langgraph langchain numexpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "abbd6948-e9a3-47ca-89c7-7ac2fc5eca8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abbd6948-e9a3-47ca-89c7-7ac2fc5eca8b",
        "outputId": "1f9f8e95-e85d-4afd-a974-e167f05bfaa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _get_pass(var: str):\n",
        "    if var not in os.environ:\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_get_pass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d499dad8",
      "metadata": {
        "id": "d499dad8"
      },
      "source": [
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
        "    <p style=\"padding-top: 5px;\">\n",
        "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0a8450",
      "metadata": {
        "id": "3a0a8450"
      },
      "source": [
        "## Helper Files\n",
        "\n",
        "### Math Tools\n",
        "\n",
        "Place the following code in a file called `math_tools.py` and ensure that you can import it into this notebook.\n",
        "\n",
        "<div>\n",
        "  <button type=\"button\" style=\"border: 1px solid black; border-radius: 5px; padding: 5px; background-color: lightgrey;\" onclick=\"toggleVisibility('helper-functions')\">Show/Hide Math Tools</button>\n",
        "  <div id=\"helper-functions\" style=\"display:none;\">\n",
        "    <!-- Helper functions -->\n",
        "    <pre>\n",
        "\n",
        "    import math\n",
        "    import re\n",
        "    from typing import List, Optional\n",
        "\n",
        "    import numexpr\n",
        "    from langchain.chains.openai_functions import create_structured_output_runnable\n",
        "    from langchain_core.messages import SystemMessage\n",
        "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "    from langchain_core.runnables import RunnableConfig\n",
        "    from langchain_core.tools import StructuredTool\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from pydantic import BaseModel, Field\n",
        "\n",
        "    _MATH_DESCRIPTION = (\n",
        "        \"math(problem: str, context: Optional[list[str]]) -> float:\\n\"\n",
        "        \" - Solves the provided math problem.\\n\"\n",
        "        ' - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n'\n",
        "        \" - You cannot calculate multiple expressions in one call. For instance, `math('1 + 3, 2 + 4')` does not work. \"\n",
        "        \"If you need to calculate multiple expressions, you need to call them separately like `math('1 + 3')` and then `math('2 + 4')`\\n\"\n",
        "        \" - Minimize the number of `math` actions as much as possible. For instance, instead of calling \"\n",
        "        '2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), '\n",
        "        'you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n'\n",
        "        # Context specific rules below\n",
        "        \" - You can optionally provide a list of strings as `context` to help the agent solve the problem. \"\n",
        "        \"If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n\"\n",
        "        \" - `math` action will not see the output of the previous actions unless you provide it as `context`. \"\n",
        "        \"You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n\"\n",
        "        \" - You MUST NEVER provide `search` type action's outputs as a variable in the `problem` argument. \"\n",
        "        \"This is because `search` returns a text blob that contains the information about the entity, not a number or value. \"\n",
        "        \"Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. \"\n",
        "        'For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. '\n",
        "        'Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n'\n",
        "        \" - When you ask a question about `context`, specify the units. \"\n",
        "        'For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"\\n'\n",
        "    )\n",
        "\n",
        "\n",
        "    _SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
        "\n",
        "    Question: ${{Question with math problem.}}\n",
        "    ```text\n",
        "    ${{single line mathematical expression that solves the problem}}\n",
        "    ```\n",
        "    ...numexpr.evaluate(text)...\n",
        "    ```output\n",
        "    ${{Output of running the code}}\n",
        "    ```\n",
        "    Answer: ${{Answer}}\n",
        "\n",
        "    Begin.\n",
        "\n",
        "    Question: What is 37593 * 67?\n",
        "    ExecuteCode({{code: \"37593 * 67\"}})\n",
        "    ...numexpr.evaluate(\"37593 * 67\")...\n",
        "    ```output\n",
        "    2518731\n",
        "    ```\n",
        "    Answer: 2518731\n",
        "\n",
        "    Question: 37593^(1/5)\n",
        "    ExecuteCode({{code: \"37593**(1/5)\"}})\n",
        "    ...numexpr.evaluate(\"37593**(1/5)\")...\n",
        "    ```output\n",
        "    8.222831614237718\n",
        "    ```\n",
        "    Answer: 8.222831614237718\n",
        "    \"\"\"\n",
        "\n",
        "    _ADDITIONAL_CONTEXT_PROMPT = \"\"\"The following additional context is provided from other functions.\\\n",
        "        Use it to substitute into any ${{#}} variables or other words in the problem.\\\n",
        "        \\n\\n${context}\\n\\nNote that context variables are not defined in code yet.\\\n",
        "    You must extract the relevant numbers and directly put them in code.\"\"\"\n",
        "\n",
        "\n",
        "    class ExecuteCode(BaseModel):\n",
        "        \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
        "\n",
        "        reasoning: str = Field(\n",
        "            ...,\n",
        "            description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
        "        )\n",
        "\n",
        "        code: str = Field(\n",
        "            ...,\n",
        "            description=\"The simple code expression to execute by numexpr.evaluate().\",\n",
        "        )\n",
        "\n",
        "\n",
        "    def _evaluate_expression(expression: str) -> str:\n",
        "        try:\n",
        "            local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
        "            output = str(\n",
        "                numexpr.evaluate(\n",
        "                    expression.strip(),\n",
        "                    global_dict={},  # restrict access to globals\n",
        "                    local_dict=local_dict,  # add common mathematical functions\n",
        "                )\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ValueError(\n",
        "                f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}.'\n",
        "                \" Please try again with a valid numerical expression\"\n",
        "            )\n",
        "\n",
        "        # Remove any leading and trailing brackets from the output\n",
        "        return re.sub(r\"^\\[|\\]$\", \"\", output)\n",
        "\n",
        "\n",
        "    def get_math_tool(llm: ChatOpenAI):\n",
        "        prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"system\", _SYSTEM_PROMPT),\n",
        "                (\"user\", \"{problem}\"),\n",
        "                MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
        "            ]\n",
        "        )\n",
        "        extractor = prompt | llm.with_structured_output(ExecuteCode)\n",
        "\n",
        "        def calculate_expression(\n",
        "            problem: str,\n",
        "            context: Optional[List[str]] = None,\n",
        "            config: Optional[RunnableConfig] = None,\n",
        "        ):\n",
        "            chain_input = {\"problem\": problem}\n",
        "            if context:\n",
        "                context_str = \"\\n\".join(context)\n",
        "                if context_str.strip():\n",
        "                    context_str = _ADDITIONAL_CONTEXT_PROMPT.format(\n",
        "                        context=context_str.strip()\n",
        "                    )\n",
        "                    chain_input[\"context\"] = [SystemMessage(content=context_str)]\n",
        "            code_model = extractor.invoke(chain_input, config)\n",
        "            try:\n",
        "                return _evaluate_expression(code_model.code)\n",
        "            except Exception as e:\n",
        "                return repr(e)\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "            name=\"math\",\n",
        "            func=calculate_expression,\n",
        "            description=_MATH_DESCRIPTION,\n",
        "        )\n",
        "\n",
        "</pre>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "  function toggleVisibility(id) {\n",
        "    var element = document.getElementById(id);\n",
        "    element.style.display = (element.style.display === \"none\") ? \"block\" : \"none\";\n",
        "  }\n",
        "</script>\n",
        "\n",
        "### Output Parser\n",
        "Place the following code in a file called output_parser.py and ensure that you can import it into this notebook.\n",
        "\n",
        "<div>\n",
        "  <button type=\"button\" style=\"border: 1px solid black; border-radius: 5px; padding: 5px; background-color: lightgrey;\" onclick=\"toggleVisibility('helper-functions-2')\">Show/Hide Output Parser</button>\n",
        "  <div id=\"helper-functions-2\" style=\"display:none;\">\n",
        "    <!-- Helper functions -->\n",
        "    <pre>\n",
        "\n",
        "    import ast\n",
        "    import re\n",
        "    from typing import (\n",
        "        Any,\n",
        "        Dict,\n",
        "        Iterator,\n",
        "        List,\n",
        "        Optional,\n",
        "        Sequence,\n",
        "        Tuple,\n",
        "        Union,\n",
        "    )\n",
        "\n",
        "    from langchain_core.exceptions import OutputParserException\n",
        "    from langchain_core.messages import BaseMessage\n",
        "    from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
        "    from langchain_core.runnables import RunnableConfig\n",
        "    from langchain_core.tools import BaseTool\n",
        "    from typing_extensions import TypedDict\n",
        "\n",
        "    THOUGHT_PATTERN = r\"Thought: ([^\\n]*)\"\n",
        "    ACTION_PATTERN = r\"\\n*(\\d+)\\. (\\w+)\\((.*)\\)(\\s*#\\w+\\n)?\"\n",
        "    # $1 or ${1} -> 1\n",
        "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
        "    END_OF_PLAN = \"<END_OF_PLAN>\"\n",
        "\n",
        "\n",
        "    ### Helper functions\n",
        "\n",
        "\n",
        "    def _ast_parse(arg: str) -> Any:\n",
        "        try:\n",
        "            return ast.literal_eval(arg)\n",
        "        except:  # noqa\n",
        "            return arg\n",
        "\n",
        "\n",
        "    def _parse_llm_compiler_action_args(args: str, tool: Union[str, BaseTool]) -> list[Any]:\n",
        "        \"\"\"Parse arguments from a string.\"\"\"\n",
        "        if args == \"\":\n",
        "            return ()\n",
        "        if isinstance(tool, str):\n",
        "            return ()\n",
        "        extracted_args = {}\n",
        "        tool_key = None\n",
        "        prev_idx = None\n",
        "        for key in tool.args.keys():\n",
        "            # Split if present\n",
        "            if f\"{key}=\" in args:\n",
        "                idx = args.index(f\"{key}=\")\n",
        "                if prev_idx is not None:\n",
        "                    extracted_args[tool_key] = _ast_parse(\n",
        "                        args[prev_idx:idx].strip().rstrip(\",\")\n",
        "                    )\n",
        "                args = args.split(f\"{key}=\", 1)[1]\n",
        "                tool_key = key\n",
        "                prev_idx = 0\n",
        "        if prev_idx is not None:\n",
        "            extracted_args[tool_key] = _ast_parse(\n",
        "                args[prev_idx:].strip().rstrip(\",\").rstrip(\")\")\n",
        "            )\n",
        "        return extracted_args\n",
        "\n",
        "\n",
        "    def default_dependency_rule(idx, args: str):\n",
        "        matches = re.findall(ID_PATTERN, args)\n",
        "        numbers = [int(match) for match in matches]\n",
        "        return idx in numbers\n",
        "\n",
        "\n",
        "    def _get_dependencies_from_graph(\n",
        "        idx: int, tool_name: str, args: Dict[str, Any]\n",
        "    ) -> dict[str, list[str]]:\n",
        "        \"\"\"Get dependencies from a graph.\"\"\"\n",
        "        if tool_name == \"join\":\n",
        "            return list(range(1, idx))\n",
        "        return [i for i in range(1, idx) if default_dependency_rule(i, str(args))]\n",
        "\n",
        "\n",
        "    class Task(TypedDict):\n",
        "        idx: int\n",
        "        tool: BaseTool\n",
        "        args: list\n",
        "        dependencies: Dict[str, list]\n",
        "        thought: Optional[str]\n",
        "\n",
        "\n",
        "    def instantiate_task(\n",
        "        tools: Sequence[BaseTool],\n",
        "        idx: int,\n",
        "        tool_name: str,\n",
        "        args: Union[str, Any],\n",
        "        thought: Optional[str] = None,\n",
        "    ) -> Task:\n",
        "        if tool_name == \"join\":\n",
        "            tool = \"join\"\n",
        "        else:\n",
        "            try:\n",
        "                tool = tools[[tool.name for tool in tools].index(tool_name)]\n",
        "            except ValueError as e:\n",
        "                raise OutputParserException(f\"Tool {tool_name} not found.\") from e\n",
        "        tool_args = _parse_llm_compiler_action_args(args, tool)\n",
        "        dependencies = _get_dependencies_from_graph(idx, tool_name, tool_args)\n",
        "\n",
        "        return Task(\n",
        "            idx=idx,\n",
        "            tool=tool,\n",
        "            args=tool_args,\n",
        "            dependencies=dependencies,\n",
        "            thought=thought,\n",
        "        )\n",
        "\n",
        "\n",
        "    class LLMCompilerPlanParser(BaseTransformOutputParser[dict], extra=\"allow\"):\n",
        "        \"\"\"Planning output parser.\"\"\"\n",
        "\n",
        "        tools: List[BaseTool]\n",
        "\n",
        "        def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Task]:\n",
        "            texts = []\n",
        "            # TODO: Cleanup tuple state tracking here.\n",
        "            thought = None\n",
        "            for chunk in input:\n",
        "                # Assume input is str. TODO: support vision/other formats\n",
        "                text = chunk if isinstance(chunk, str) else str(chunk.content)\n",
        "                for task, thought in self.ingest_token(text, texts, thought):\n",
        "                    yield task\n",
        "            # Final possible task\n",
        "            if texts:\n",
        "                task, _ = self._parse_task(\"\".join(texts), thought)\n",
        "                if task:\n",
        "                    yield task\n",
        "\n",
        "        def parse(self, text: str) -> List[Task]:\n",
        "            return list(self._transform([text]))\n",
        "\n",
        "        def stream(\n",
        "            self,\n",
        "            input: str | BaseMessage,\n",
        "            config: RunnableConfig | None = None,\n",
        "            **kwargs: Any | None,\n",
        "        ) -> Iterator[Task]:\n",
        "            yield from self.transform([input], config, **kwargs)\n",
        "\n",
        "        def ingest_token(\n",
        "            self, token: str, buffer: List[str], thought: Optional[str]\n",
        "        ) -> Iterator[Tuple[Optional[Task], str]]:\n",
        "            buffer.append(token)\n",
        "            if \"\\n\" in token:\n",
        "                buffer_ = \"\".join(buffer).split(\"\\n\")\n",
        "                suffix = buffer_[-1]\n",
        "                for line in buffer_[:-1]:\n",
        "                    task, thought = self._parse_task(line, thought)\n",
        "                    if task:\n",
        "                        yield task, thought\n",
        "                buffer.clear()\n",
        "                buffer.append(suffix)\n",
        "\n",
        "        def _parse_task(self, line: str, thought: Optional[str] = None):\n",
        "            task = None\n",
        "            if match := re.match(THOUGHT_PATTERN, line):\n",
        "                # Optionally, action can be preceded by a thought\n",
        "                thought = match.group(1)\n",
        "            elif match := re.match(ACTION_PATTERN, line):\n",
        "                # if action is parsed, return the task, and clear the buffer\n",
        "                idx, tool_name, args, _ = match.groups()\n",
        "                idx = int(idx)\n",
        "                task = instantiate_task(\n",
        "                    tools=self.tools,\n",
        "                    idx=idx,\n",
        "                    tool_name=tool_name,\n",
        "                    args=args,\n",
        "                    thought=thought,\n",
        "                )\n",
        "                thought = None\n",
        "            # Else it is just dropped\n",
        "            return task, thought\n",
        "\n",
        "\n",
        "</pre>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "  function toggleVisibility(id) {\n",
        "    var element = document.getElementById(id);\n",
        "    element.style.display = (element.style.display === \"none\") ? \"block\" : \"none\";\n",
        "  }\n",
        "</script>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a61b48ee-8c6f-4863-913a-676f659287de",
      "metadata": {
        "id": "a61b48ee-8c6f-4863-913a-676f659287de"
      },
      "source": [
        "## Define Tools\n",
        "\n",
        "We'll first define the tools for the agent to use in our demo. We'll give it the class search engine + calculator combo.\n",
        "\n",
        "If you don't want to sign up for tavily, you can replace it with the free [DuckDuckGo](https://python.langchain.com/docs/integrations/tools/ddg/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OLB5H4CE7inu",
        "outputId": "c962f02c-cb2f-4292-e33b-edfc3692bae5"
      },
      "id": "OLB5H4CE7inu",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.41)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e7476bb2-1a51-42f6-b7ae-82a0300bbf84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7476bb2-1a51-42f6-b7ae-82a0300bbf84",
        "outputId": "1512dc46-71c3-4b79-ddc2-5fc98f7570ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TAVILY_API_KEY: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-4-turbo-preview since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from math_tools import get_math_tool\n",
        "\n",
        "_get_pass(\"TAVILY_API_KEY\")\n",
        "\n",
        "calculate = get_math_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
        "search = TavilySearchResults(\n",
        "    max_results=1,\n",
        "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
        ")\n",
        "\n",
        "tools = [search, calculate]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "152eecf3-6bef-4718-af71-a0b3c5a3b009",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "152eecf3-6bef-4718-af71-a0b3c5a3b009",
        "outputId": "e58b8de8-d660-4db7-9447-065bdb2d0720"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'37'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "calculate.invoke(\n",
        "    {\n",
        "        \"problem\": \"What's the temp of sf + 5?\",\n",
        "        \"context\": [\"Thet empreature of sf is 32 degrees\"],\n",
        "    }\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1abdedbd-d81b-4ee9-b46f-f29439ed1350",
      "metadata": {
        "id": "1abdedbd-d81b-4ee9-b46f-f29439ed1350"
      },
      "source": [
        "## Planner\n",
        "\n",
        "\n",
        "Largely adapted from [the original source code](https://github.com/SqueezeAILab/LLMCompiler/blob/main/src/llm_compiler/output_parser.py), the planner  accepts the input question and generates a task list to execute.\n",
        "\n",
        "If it is provided with a previous plan, it is instructed to re-plan, which is useful if, upon completion of the first batch of tasks, the agent must take more actions.\n",
        "\n",
        "The code below composes constructs the prompt template for the planner and composes it with LLM and output parser, defined in `output_parser.py`. The output parser processes a task list in the following form:\n",
        "\n",
        "```plaintext\n",
        "1. tool_1(arg1=\"arg1\", arg2=3.5, ...)\n",
        "Thought: I then want to find out Y by using tool_2\n",
        "2. tool_2(arg1=\"\", arg2=\"${1}\")'\n",
        "3. join()<END_OF_PLAN>\"\n",
        "```\n",
        "\n",
        "The \"Thought\" lines are optional. The `${#}` placeholders are variables. These are used to route tool (task) outputs to other tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "15dd9639-691f-4906-9012-83fd6e9ac126",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15dd9639-691f-4906-9012-83fd6e9ac126",
        "outputId": "5e2151b2-2df6-4ace-b144-7ec12666e455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
            "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
            "\n",
            " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
            " - join should always be the last action in the plan, and will be called in two scenarios:\n",
            "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
            "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
            " - Each action described above contains input/output types and description.\n",
            "    - You must strictly adhere to the input and output types for each action.\n",
            "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
            " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
            " - Each action MUST have a unique ID, which is strictly increasing.\n",
            " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
            " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
            " - Ensure the plan maximizes parallelizability.\n",
            " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
            " - Never introduce new actions other than the ones provided.\n",
            "\n",
            "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
            "\n",
            "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
            "\n",
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
            "idx. tool(arg_name=args)\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import (\n",
        "    BaseMessage,\n",
        "    FunctionMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "from langchain_core.tools import BaseTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from output_parser import LLMCompilerPlanParser, Task\n",
        "\n",
        "prompt = hub.pull(\"wfh/llm-compiler\")\n",
        "print(prompt.pretty_print())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "45689d40-d8df-4316-a121-6ea9c87d2efe",
      "metadata": {
        "id": "45689d40-d8df-4316-a121-6ea9c87d2efe"
      },
      "outputs": [],
      "source": [
        "def create_planner(\n",
        "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
        "):\n",
        "    tool_descriptions = \"\\n\".join(\n",
        "        f\"{i+1}. {tool.description}\\n\"\n",
        "        for i, tool in enumerate(\n",
        "            tools\n",
        "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
        "    )\n",
        "    planner_prompt = base_prompt.partial(\n",
        "        replan=\"\",\n",
        "        num_tools=len(tools)\n",
        "        + 1,  # Add one because we're adding the join() tool at the end.\n",
        "        tool_descriptions=tool_descriptions,\n",
        "    )\n",
        "    replanner_prompt = base_prompt.partial(\n",
        "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
        "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
        "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
        "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
        "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
        "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
        "        num_tools=len(tools) + 1,\n",
        "        tool_descriptions=tool_descriptions,\n",
        "    )\n",
        "\n",
        "    def should_replan(state: list):\n",
        "        # Context is passed as a system message\n",
        "        return isinstance(state[-1], SystemMessage)\n",
        "\n",
        "    def wrap_messages(state: list):\n",
        "        return {\"messages\": state}\n",
        "\n",
        "    def wrap_and_get_last_index(state: list):\n",
        "        next_task = 0\n",
        "        for message in state[::-1]:\n",
        "            if isinstance(message, FunctionMessage):\n",
        "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
        "                break\n",
        "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
        "        return {\"messages\": state}\n",
        "\n",
        "    return (\n",
        "        RunnableBranch(\n",
        "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
        "            wrap_messages | planner_prompt,\n",
        "        )\n",
        "        | llm\n",
        "        | LLMCompilerPlanParser(tools=tools)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "bbdcb57b-5362-4b9e-88db-fb3fae443fb0",
      "metadata": {
        "id": "bbdcb57b-5362-4b9e-88db-fb3fae443fb0"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
        "# This is the primary \"agent\" in our application\n",
        "planner = create_planner(llm, tools, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "730490c6-6e3a-4173-82a1-9eb9d5eeff20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "730490c6-6e3a-4173-82a1-9eb9d5eeff20",
        "outputId": "62ea3a6b-94fc-4097-ad53-9a053a8280a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')) {'query': 'current temperature in San Francisco'}\n",
            "---\n",
            "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'langchain_core.utils.pydantic.math'> func=<function get_math_tool.<locals>.calculate_expression at 0x78e99399ff60> {'problem': 'raise to the power of 3', 'context': ['$1']}\n",
            "---\n",
            "join ()\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
        "\n",
        "for task in planner.stream([HumanMessage(content=example_question)]):\n",
        "    print(task[\"tool\"], task[\"args\"])\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0e795f-61ff-4553-9823-23e7624ca180",
      "metadata": {
        "id": "5d0e795f-61ff-4553-9823-23e7624ca180"
      },
      "source": [
        "## Task Fetching Unit\n",
        "\n",
        "This component schedules the tasks. It receives a stream of tools of the following format:\n",
        "\n",
        "```typescript\n",
        "{\n",
        "    tool: BaseTool,\n",
        "    dependencies: number[],\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "The basic idea is to begin executing tools as soon as their dependencies are met. This is done through multi-threading. We will combine the task fetching unit and executor below:\n",
        "\n",
        "![diagram](attachment:692589f3-0ee2-459c-82d3-2817e637ddd4.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c1fbafdd-42d4-4575-8466-e5951cee71f4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "c1fbafdd-42d4-4575-8466-e5951cee71f4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "from typing import Any, Dict, Iterable, List, Union\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    chain as as_runnable,\n",
        ")\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
        "    # Get all previous tool responses\n",
        "    results = {}\n",
        "    for message in messages[::-1]:\n",
        "        if isinstance(message, FunctionMessage):\n",
        "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
        "    return results\n",
        "\n",
        "\n",
        "class SchedulerInput(TypedDict):\n",
        "    messages: List[BaseMessage]\n",
        "    tasks: Iterable[Task]\n",
        "\n",
        "\n",
        "def _execute_task(task, observations, config):\n",
        "    tool_to_use = task[\"tool\"]\n",
        "    if isinstance(tool_to_use, str):\n",
        "        return tool_to_use\n",
        "    args = task[\"args\"]\n",
        "    try:\n",
        "        if isinstance(args, str):\n",
        "            resolved_args = _resolve_arg(args, observations)\n",
        "        elif isinstance(args, dict):\n",
        "            resolved_args = {\n",
        "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
        "            }\n",
        "        else:\n",
        "            # This will likely fail\n",
        "            resolved_args = args\n",
        "    except Exception as e:\n",
        "        return (\n",
        "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
        "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
        "        )\n",
        "    try:\n",
        "        return tool_to_use.invoke(resolved_args, config)\n",
        "    except Exception as e:\n",
        "        return (\n",
        "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
        "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
        "    # $1 or ${1} -> 1\n",
        "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
        "\n",
        "    def replace_match(match):\n",
        "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
        "\n",
        "        # Return the match group, in this case the index, from the string. This is the index\n",
        "        # number we get back.\n",
        "        idx = int(match.group(1))\n",
        "        return str(observations.get(idx, match.group(0)))\n",
        "\n",
        "    # For dependencies on other tasks\n",
        "    if isinstance(arg, str):\n",
        "        return re.sub(ID_PATTERN, replace_match, arg)\n",
        "    elif isinstance(arg, list):\n",
        "        return [_resolve_arg(a, observations) for a in arg]\n",
        "    else:\n",
        "        return str(arg)\n",
        "\n",
        "\n",
        "@as_runnable\n",
        "def schedule_task(task_inputs, config):\n",
        "    task: Task = task_inputs[\"task\"]\n",
        "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
        "    try:\n",
        "        observation = _execute_task(task, observations, config)\n",
        "    except Exception:\n",
        "        import traceback\n",
        "\n",
        "        observation = traceback.format_exception()  # repr(e) +\n",
        "    observations[task[\"idx\"]] = observation\n",
        "\n",
        "\n",
        "def schedule_pending_task(\n",
        "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
        "):\n",
        "    while True:\n",
        "        deps = task[\"dependencies\"]\n",
        "        if deps and (any([dep not in observations for dep in deps])):\n",
        "            # Dependencies not yet satisfied\n",
        "            time.sleep(retry_after)\n",
        "            continue\n",
        "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
        "        break\n",
        "\n",
        "\n",
        "@as_runnable\n",
        "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
        "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
        "    # For streaming, we are making a few simplifying assumption:\n",
        "    # 1. The LLM does not create cyclic dependencies\n",
        "    # 2. That the LLM will not generate tasks with future deps\n",
        "    # If this ceases to be a good assumption, you can either\n",
        "    # adjust to do a proper topological sort (not-stream)\n",
        "    # or use a more complicated data structure\n",
        "    tasks = scheduler_input[\"tasks\"]\n",
        "    args_for_tasks = {}\n",
        "    messages = scheduler_input[\"messages\"]\n",
        "    # If we are re-planning, we may have calls that depend on previous\n",
        "    # plans. Start with those.\n",
        "    observations = _get_observations(messages)\n",
        "    task_names = {}\n",
        "    originals = set(observations)\n",
        "    # ^^ We assume each task inserts a different key above to\n",
        "    # avoid race conditions...\n",
        "    futures = []\n",
        "    retry_after = 0.25  # Retry every quarter second\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for task in tasks:\n",
        "            deps = task[\"dependencies\"]\n",
        "            task_names[task[\"idx\"]] = (\n",
        "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
        "            )\n",
        "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
        "            if (\n",
        "                # Depends on other tasks\n",
        "                deps and (any([dep not in observations for dep in deps]))\n",
        "            ):\n",
        "                futures.append(\n",
        "                    executor.submit(\n",
        "                        schedule_pending_task, task, observations, retry_after\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                # No deps or all deps satisfied\n",
        "                # can schedule now\n",
        "                schedule_task.invoke(dict(task=task, observations=observations))\n",
        "                # futures.append(executor.submit(schedule_task.invoke, dict(task=task, observations=observations)))\n",
        "\n",
        "        # All tasks have been submitted or enqueued\n",
        "        # Wait for them to complete\n",
        "        wait(futures)\n",
        "    # Convert observations to new tool messages to add to the state\n",
        "    new_observations = {\n",
        "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
        "        for k in sorted(observations.keys() - originals)\n",
        "    }\n",
        "    tool_messages = [\n",
        "        FunctionMessage(\n",
        "            name=name,\n",
        "            content=str(obs),\n",
        "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
        "            tool_call_id=k,\n",
        "        )\n",
        "        for k, (name, task_args, obs) in new_observations.items()\n",
        "    ]\n",
        "    return tool_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "052f6b16-103a-40e9-94dd-8fcc37e77ba4",
      "metadata": {
        "id": "052f6b16-103a-40e9-94dd-8fcc37e77ba4"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "@as_runnable\n",
        "def plan_and_schedule(state):\n",
        "    messages = state[\"messages\"]\n",
        "    tasks = planner.stream(messages)\n",
        "    # Begin executing the planner immediately\n",
        "    try:\n",
        "        tasks = itertools.chain([next(tasks)], tasks)\n",
        "    except StopIteration:\n",
        "        # Handle the case where tasks is empty.\n",
        "        tasks = iter([])\n",
        "    scheduled_tasks = schedule_tasks.invoke(\n",
        "        {\n",
        "            \"messages\": messages,\n",
        "            \"tasks\": tasks,\n",
        "        }\n",
        "    )\n",
        "    return {\"messages\": scheduled_tasks}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efa15ae-817a-48c6-86ed-16bc112fedc5",
      "metadata": {
        "id": "9efa15ae-817a-48c6-86ed-16bc112fedc5"
      },
      "source": [
        "### Example Plan\n",
        "\n",
        "We still haven't introduced any cycles in our computation graph, so this is all easily expressed in LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "55142257-2674-4a47-988e-0d2810917329",
      "metadata": {
        "id": "55142257-2674-4a47-988e-0d2810917329"
      },
      "outputs": [],
      "source": [
        "tool_messages = plan_and_schedule.invoke(\n",
        "    {\"messages\": [HumanMessage(content=example_question)]}\n",
        ")[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a98e0525-2fcf-4fa1-baf6-79858bb8a6bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a98e0525-2fcf-4fa1-baf6-79858bb8a6bd",
        "outputId": "ac189ab7-7c05-477b-d7c1-81af957b0de6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FunctionMessage(content=\"[{'title': 'San Francisco, CA Weather Conditions | Weather Underground', 'url': 'https://www.wunderground.com/weather/us/ca/san-francisco', 'content': 'San Francisco, CA Weather Conditions | Weather Underground San Francisco, CA Weather Conditions_star_rate__home_ 49\\\\xa0°F South of Market Station|Report Report Station You are about to report this weather station for bad data. Personal Weather Station Nearby Weather Stations Nearby Weather Stations Weather Data APIs The time period when the sun is no more than 6 degrees below the horizon at either sunrise or sunset. The horizon should be clearly defined and the brightest stars should be visible under good atmospheric conditions (i.e. no moonlight, or other lights). The sun does not contribute to the illumination of the sky before this time in the morning, or after this time in the evening.', 'score': 0.5090084}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in San Francisco'}}, response_metadata={}, name='tavily_search_results_json', tool_call_id=1),\n",
              " FunctionMessage(content='117649', additional_kwargs={'idx': 2, 'args': {'problem': '$1 raised to the 3rd power', 'context': ['temperature in SF']}}, response_metadata={}, name='math', tool_call_id=2),\n",
              " FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', tool_call_id=3)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "tool_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563d5311-55f0-4ca1-afbd-01fd970cf3e3",
      "metadata": {
        "id": "563d5311-55f0-4ca1-afbd-01fd970cf3e3"
      },
      "source": [
        "## Joiner\n",
        "\n",
        "So now we have the planning and initial execution done. We need a component to process these outputs and either:\n",
        "\n",
        "1. Respond with the correct answer.\n",
        "2. Loop with a new plan.\n",
        "\n",
        "The paper refers to this as the \"joiner\". It's another LLM call. We are using function calling to improve parsing reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4d2b50-5e10-4111-996f-e58c694d488f",
      "metadata": {
        "id": "ad4d2b50-5e10-4111-996f-e58c694d488f"
      },
      "source": [
        "<div class=\"admonition note\">\n",
        "    <p class=\"admonition-title\">Using Pydantic with LangChain</p>\n",
        "    <p>\n",
        "        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "942dab42-ad42-4ba2-90d5-49edbe4fae68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "942dab42-ad42-4ba2-90d5-49edbe4fae68",
        "outputId": "adcabf3b-803c-45b6-b7a4-aff97e879d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class FinalResponse(BaseModel):\n",
        "    \"\"\"The final response/answer.\"\"\"\n",
        "\n",
        "    response: str\n",
        "\n",
        "\n",
        "class Replan(BaseModel):\n",
        "    feedback: str = Field(\n",
        "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class JoinOutputs(BaseModel):\n",
        "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
        "\n",
        "    thought: str = Field(\n",
        "        description=\"The chain of thought reasoning for the selected action\"\n",
        "    )\n",
        "    action: Union[FinalResponse, Replan]\n",
        "\n",
        "\n",
        "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
        "    examples=\"\"\n",
        ")  # You can optionally add examples\n",
        "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
        "\n",
        "runnable = joiner_prompt | llm.with_structured_output(\n",
        "    JoinOutputs, method=\"function_calling\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb50c4cd-947c-4a5d-a9f7-f0d92a10600f",
      "metadata": {
        "id": "fb50c4cd-947c-4a5d-a9f7-f0d92a10600f"
      },
      "source": [
        "We will select only the most recent messages in the state, and format the output to be more useful for\n",
        "the planner, should the agent need to loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "951a33cf-2a05-4a33-899a-0ab1d97122fa",
      "metadata": {
        "id": "951a33cf-2a05-4a33-899a-0ab1d97122fa"
      },
      "outputs": [],
      "source": [
        "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
        "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
        "    if isinstance(decision.action, Replan):\n",
        "        return {\n",
        "            \"messages\": response\n",
        "            + [\n",
        "                SystemMessage(\n",
        "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
        "                )\n",
        "            ]\n",
        "        }\n",
        "    else:\n",
        "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
        "\n",
        "\n",
        "def select_recent_messages(state) -> dict:\n",
        "    messages = state[\"messages\"]\n",
        "    selected = []\n",
        "    for msg in messages[::-1]:\n",
        "        selected.append(msg)\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            break\n",
        "    return {\"messages\": selected[::-1]}\n",
        "\n",
        "\n",
        "joiner = select_recent_messages | runnable | _parse_joiner_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1e49d4b1-8266-4520-a566-1448b1c31c8f",
      "metadata": {
        "id": "1e49d4b1-8266-4520-a566-1448b1c31c8f"
      },
      "outputs": [],
      "source": [
        "input_messages = [HumanMessage(content=example_question)] + tool_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "31854dfd-b82f-4c24-9b58-6bae66777909",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31854dfd-b82f-4c24-9b58-6bae66777909",
        "outputId": "1f511d21-0a1c-43e2-e633-596a0acc6b32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='Thought: The current temperature in San Francisco is provided as 49°F, and the math calculation result of raising this temperature to the 3rd power is 117649.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The temperature in San Francisco raised to the 3rd power is 117,649.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "joiner.invoke({\"messages\": input_messages})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b099e5ee-2c23-47d9-9387-0f64e02627d3",
      "metadata": {
        "id": "b099e5ee-2c23-47d9-9387-0f64e02627d3"
      },
      "source": [
        "## Compose using LangGraph\n",
        "\n",
        "We'll define the agent as a stateful graph, with the main nodes being:\n",
        "\n",
        "1. Plan and execute (the DAG from the first step above)\n",
        "2. Join: determine if we should finish or replan\n",
        "3. Recontextualize: update the graph state based on the output from the joiner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "768b5f11-e3d2-47be-8143-a7dcd8765243",
      "metadata": {
        "id": "768b5f11-e3d2-47be-8143-a7dcd8765243"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# 1.  Define vertices\n",
        "# We defined plan_and_schedule above already\n",
        "# Assign each node to a state variable to update\n",
        "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
        "graph_builder.add_node(\"join\", joiner)\n",
        "\n",
        "\n",
        "## Define edges\n",
        "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
        "\n",
        "### This condition determines looping logic\n",
        "\n",
        "\n",
        "def should_continue(state):\n",
        "    messages = state[\"messages\"]\n",
        "    if isinstance(messages[-1], AIMessage):\n",
        "        return END\n",
        "    return \"plan_and_schedule\"\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"join\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        ")\n",
        "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
        "chain = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8c9849-8531-463d-a0ef-dcc3d9888b2d",
      "metadata": {
        "id": "9f8c9849-8531-463d-a0ef-dcc3d9888b2d"
      },
      "source": [
        "### Simple question\n",
        "\n",
        "Let's ask a simple question of the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5bc4584a-e31c-4065-805e-76a6db30676a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bc4584a-e31c-4065-805e-76a6db30676a",
        "outputId": "6f222805-aac3-48ea-cd42-6ef847b55f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan_and_schedule': {'messages': [FunctionMessage(content='[{\\'title\\': \\'What is the gross domestic product (GDP) in New York? - USAFacts\\', \\'url\\': \\'https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/\\', \\'content\\': \"What is the gross domestic product (GDP) in New York? What is the gross domestic product (GDP) in New York? Gross domestic product (GDP) measures the value of goods and services a country or state produces — it’s the sum of consumer spending, business investment, government spending, and net exports. As of 2023, the real GDP was $1.8 trillion. Real GDP in New York, adjusted for inflation (chained 2017 dollars) GDP and the economic experience vary by location due to factors like cost of living, population density, workforce education, and the area’s main industries. In 2023, New York\\'s real (that is, inflation-adjusted) GDP per person was 1st out of all 50 states. In 2023, New York ranked 1st in state GDP per person.\", \\'score\\': 0.89997756}]', additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York'}}, response_metadata={}, name='tavily_search_results_json', id='4c78934f-c7c6-4c69-8272-47f9783f93ff', tool_call_id=1)]}}\n",
            "---\n",
            "{'join': {'messages': [AIMessage(content=\"Thought: The search results provide the answer to the query. The gross domestic product (GDP) of New York in 2023 is $1.8 trillion. This information directly answers the user's question.\", additional_kwargs={}, response_metadata={}, id='5bc18600-abaf-4a2b-83ce-ec844d3015c6'), AIMessage(content='The GDP of New York in 2023 is $1.8 trillion.', additional_kwargs={}, response_metadata={}, id='9f74fdbd-f0fe-4942-9723-c903514c2c43')]}}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "for step in chain.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
        "):\n",
        "    print(step)\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b96efd08-5314-44f0-a694-3073b638adad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b96efd08-5314-44f0-a694-3073b638adad",
        "outputId": "3f603baa-ea3c-4318-ce62-1202a127866b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The GDP of New York in 2023 is $1.8 trillion.\n"
          ]
        }
      ],
      "source": [
        "# Final answer\n",
        "print(step[\"join\"][\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c65ef5-b4b2-4ab2-8c78-a551da7819b9",
      "metadata": {
        "id": "33c65ef5-b4b2-4ab2-8c78-a551da7819b9"
      },
      "source": [
        "### Multi-hop question\n",
        "\n",
        "This question requires that the agent perform multiple searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "0b3a0916-d8ca-4092-b91c-d9e2b05259d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3a0916-d8ca-4092-b91c-d9e2b05259d8",
        "outputId": "17ebc407-291c-4cab-894e-c6339314344b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'title': 'Oldest parrot ever | Guinness World Records', 'url': 'https://guinnessworldrecords.com/world-records/442525-oldest-parrot-ever', 'content': 'Oldest parrot ever | Guinness World Records Latest news Latest videos Records showcaseMeet our icons The oldest parrot ever is Cookie, a Major Mitchell’s cockatoo (Cacatua leadbeateri) who was at least 82 years and 88 days old when he passed away on 27 August 2016. Because the precise day in May 1934 was not documented, the age calculation in this record is taken from the final day of the documented month of arrival at the zoo (although the parrot may have been a full 30 days older if his arrival had been on 1st May). Comments below may relate to previous holders of this record. Related Records Cookie Policy GWR for Business: Contact an Account Manager', 'score': 0.9158166}]\", additional_kwargs={'idx': 1, 'args': {'query': 'oldest parrot alive'}}, response_metadata={}, name='tavily_search_results_json', id='1ace9d6f-5557-4d63-aa33-e5b660b792fb', tool_call_id=1), FunctionMessage(content='[{\\'title\\': \\'How Long Do Parrots Live? - PetMD\\', \\'url\\': \\'https://www.petmd.com/bird/how-long-do-parrots-live\\', \\'content\\': \"How Long Do Parrots Live? Parrots may live anywhere from 15 to over 50 years depending on their unique species and level of care. So, how long do parrots live, exactly? Larger birds, like macaws and grey parrots, can live 25–50 years. Improving care, including providing a balanced diet, is the best way to increase a parrot\\'s lifespan. How long do parrots live in captivity? Why do parrots live so long? How old is the oldest living parrot? How Long do Pet Parrots and Other Birds Live?. A Quick Reference Guid to Unique Pet Species: Pionus Parrots Pet Care. How Long do Pet Parrots and Other Birds Live?. A Quick Reference Guid to Unique Pet Species: Pionus Parrots Pet Care.\", \\'score\\': 0.84246206}]', additional_kwargs={'idx': 2, 'args': {'query': 'average lifespan of a parrot'}}, response_metadata={}, name='tavily_search_results_json', id='aa91f7c0-c59f-4c1a-b1a1-35f61c2bba47', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='e1f983d5-24bf-48d7-8aac-94d1fb211cec', tool_call_id=3)]}}\n",
            "---\n",
            "{'join': {'messages': [AIMessage(content='Thought: The oldest parrot ever was Cookie, a Major Mitchell’s cockatoo, who was 82 years and 88 days old at the time of passing. The average lifespan of larger parrots, like macaws and grey parrots, can be from 25 to 50 years. This provides the necessary information to give a specific answer and a general comparison to the average lifespan.', additional_kwargs={}, response_metadata={}, id='e2df199a-fd55-4d99-a0af-c59c55186210'), AIMessage(content='The oldest parrot ever recorded was Cookie, a Major Mitchell’s cockatoo, who lived to be 82 years and 88 days old. Comparatively, the lifespan of larger parrot species, such as macaws and grey parrots, generally ranges from 25 to 50 years. Therefore, Cookie lived significantly longer than the average for these types of parrots.', additional_kwargs={}, response_metadata={}, id='4c2c09ee-8e15-42bc-8878-b82e892fe38a')]}}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "steps = chain.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
        "            )\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"recursion_limit\": 100,\n",
        "    },\n",
        ")\n",
        "for step in steps:\n",
        "    print(step)\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6c65c414-7668-4fdf-ba97-f42f659b1317",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c65c414-7668-4fdf-ba97-f42f659b1317",
        "outputId": "a6e2bedb-e0b4-451c-cfdc-6f5441bfdc02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The oldest parrot ever recorded was Cookie, a Major Mitchell’s cockatoo, who lived to be 82 years and 88 days old. Comparatively, the lifespan of larger parrot species, such as macaws and grey parrots, generally ranges from 25 to 50 years. Therefore, Cookie lived significantly longer than the average for these types of parrots.\n"
          ]
        }
      ],
      "source": [
        "# Final answer\n",
        "print(step[\"join\"][\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b859bc7-1a85-4d35-b57b-f67c87282403",
      "metadata": {
        "id": "1b859bc7-1a85-4d35-b57b-f67c87282403"
      },
      "source": [
        "### Multi-step  math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "38d3ea91-59ba-4267-8060-ed75bbc840c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38d3ea91-59ba-4267-8060-ed75bbc840c6",
        "outputId": "b1c91ddd-1bb3-4ec7-fce6-63c776c13f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan_and_schedule': {'messages': [FunctionMessage(content='3307.0', additional_kwargs={'idx': 1, 'args': {'problem': '((3*(4+5)/0.5)+3245) + 8'}}, response_metadata={}, name='math', id='40a4c0e0-2b84-40fe-9447-83ab125e982d', tool_call_id=1), FunctionMessage(content='7.565011820330969', additional_kwargs={'idx': 2, 'args': {'problem': '32/4.23'}}, response_metadata={}, name='math', id='e408bcfe-4e9b-484d-8ce0-8c731122d165', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='28991094-494e-4161-a5a8-f25722d6f072', tool_call_id=3)]}}\n",
            "{'join': {'messages': [AIMessage(content='Thought: We have the results of both calculations. The first calculation is 3307.0, and the second is 7.565011820330969. To find the sum, we just need to add these two results together.', additional_kwargs={}, response_metadata={}, id='9a2c1eb8-4d64-4535-baf5-874d26268a8e'), AIMessage(content='The result of ((3*(4+5)/0.5)+3245) + 8 is 3307.0, the result of 32/4.23 is approximately 7.57, and the sum of those two values is approximately 3314.57.', additional_kwargs={}, response_metadata={}, id='e77786d3-1f8c-4d87-9e61-5ab22fdb41e2')]}}\n"
          ]
        }
      ],
      "source": [
        "for step in chain.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"What's ((3*(4+5)/0.5)+3245) + 8? What's 32/4.23? What's the sum of those two values?\"\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    print(step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a6cf5fe0-f178-4197-950f-257711bff8d2",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6cf5fe0-f178-4197-950f-257711bff8d2",
        "outputId": "6a8055d8-ba8b-42cc-bfa5-2fc79ee4a760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of ((3*(4+5)/0.5)+3245) + 8 is 3307.0, the result of 32/4.23 is approximately 7.57, and the sum of those two values is approximately 3314.57.\n"
          ]
        }
      ],
      "source": [
        "# Final answer\n",
        "print(step[\"join\"][\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9487866",
      "metadata": {
        "id": "f9487866"
      },
      "source": [
        "### Complex Replanning Example\n",
        "\n",
        "This question is likely to prompt the Replan functionality, but it may need to be run multiple times to see this in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "391d6931",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "391d6931",
        "outputId": "a1af3e67-fe43-49e8-e5ec-25b3be562e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'title': 'Weather for Tokyo, Japan - Time and Date', 'url': 'https://www.timeanddate.com/weather/japan/tokyo', 'content': 'Home \\\\xa0 Weather \\\\xa0 Japan \\\\xa0 Tokyo Weather in Tokyo, Japan Weather 32\\\\xa0°F Forecast: 51 / 31\\\\xa0°F Current Time:   Feb 10, 2025 at 8:22:57 am See more hour-by-hour weather Forecast for the next 48 hours Forecast                           ↑   S * Updated Monday, February 10, 2025 4:42:53 am Tokyo time - Weather by CustomWeather, © 2025 52 / 34\\\\xa0°F More weather last week 39\\\\xa0°F 30\\\\xa0°F 32\\\\xa0°F More weather in Japan 51 / 31\\\\xa0°F 53 / 32\\\\xa0°F 57 / 29\\\\xa0°F 58 / 39\\\\xa0°F 56 / 37\\\\xa0°F 61 / 40\\\\xa0°F 53 / 38\\\\xa0°F 51 / 36\\\\xa0°F 52 / 41\\\\xa0°F 50 / 39\\\\xa0°F Sun & Moon times precise to the second. Privacy Policy Weather', 'score': 0.81770587}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in Tokyo'}}, response_metadata={}, name='tavily_search_results_json', id='44a89453-1b3b-43cf-99e0-8cb418495e2a', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='c85f8cab-db98-4219-8c29-0f5f63134190', tool_call_id=2)]}}\n",
            "{'join': {'messages': [AIMessage(content='Thought: The search results provide the current temperature in Tokyo, which is 32°F as of February 10, 2025, at 8:22:57 am. This information is sufficient to create a flashcard summarizing the current temperature in Tokyo.', additional_kwargs={}, response_metadata={}, id='1e808995-ea8f-49f2-9603-c59d227cca61'), AIMessage(content='**Flashcard Summary:**\\n- **Location:** Tokyo, Japan\\n- **Current Temperature:** 32°F\\n- **Date & Time:** February 10, 2025, at 8:22:57 am\\n\\nStay warm in Tokyo!', additional_kwargs={}, response_metadata={}, id='c048adfe-280a-4f47-a538-dfb7563269b9')]}}\n"
          ]
        }
      ],
      "source": [
        "for step in chain.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Find the current temperature in Tokyo, then, respond with a flashcard summarizing this information\"\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    print(step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c647d5f3-5e00-4449-9cec-5a9f438c9cff",
      "metadata": {
        "id": "c647d5f3-5e00-4449-9cec-5a9f438c9cff"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congrats on building your first LLMCompiler agent! I'll leave you with some known limitations to the implementation above:\n",
        "\n",
        "1. The planner output parsing format is fragile if your function requires more than 1 or 2 arguments. We could make it more robust by using streaming tool calling.\n",
        "2. Variable substitution is fragile in the example above. It could be made more robust by using a fine-tuned model and a more robust syntax (using e.g., Lark or a tool calling schema)\n",
        "3. The state can grow quite long if you require multiple re-planning runs. To handle, you could add a message compressor once you go above a certain token limit.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}